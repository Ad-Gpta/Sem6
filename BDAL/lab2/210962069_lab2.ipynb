{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple PySpark Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Exercises:\n",
    "\n",
    "1) Implement a PySpark script that applies transformations like filter and withColumn on a DataFrame.\n",
    "\n",
    "2) Write a PySpark script that performs actions like count and show on a DataFrame.\n",
    "\n",
    "3) Demonstrate how to perform basic aggregations (e.g., sum, average) on a PySpark DataFrame.\n",
    "\n",
    "4) Show how to write a PySpark DataFrame to a CSV file.\n",
    "\n",
    "5) Implement wordcount program in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#creating a spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#create dataframe\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"A\"),\n",
    "    (2, \"B\"),\n",
    "    (3, \"C\"),\n",
    "    (4, \"D\"),\n",
    "    (5, \"E\")]\n",
    "    ,[\"id\",\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count and show on a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|   A|\n",
      "|  2|   B|\n",
      "|  3|   C|\n",
      "|  4|   D|\n",
      "|  5|   E|\n",
      "+---+----+\n",
      "\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply transformations like filter and withColumn on a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  1|   A|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.filter(df[\"name\"].startswith(\"A\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  1|   A| 10|\n",
      "|  2|   B| 20|\n",
      "|  3|   C| 30|\n",
      "|  4|   D| 40|\n",
      "|  5|   E| 50|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"age\", df[\"id\"]*10)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic aggregations (e.g., sum, average) on a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sum(age)|\n",
      "+--------+\n",
      "|     150|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df2.select(sum(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    30.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df2.select(avg(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcount program in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     word|count|\n",
      "+---------+-----+\n",
      "|       in|    3|\n",
      "|  pyspark|    3|\n",
      "|wordcount|    3|\n",
      "|      for|    2|\n",
      "|       is|    2|\n",
      "|  testing|    2|\n",
      "|   sample|    2|\n",
      "|     This|    2|\n",
      "|     text|    2|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode,split,col\n",
    "\n",
    "df=spark.read.text(\"/home/lplab/Desktop/210962069/BDAL/lab2/text\")\n",
    "\n",
    "#Apply Split, Explode and groupBy to get count()\n",
    "df_count=(\n",
    "  df.withColumn('word', explode(split(col('value'), ' ')))\n",
    "    .groupBy('word')\n",
    "    .count()\n",
    "    .sort('count', ascending=False)\n",
    ")\n",
    "\n",
    "#Display Output\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write PySpark DataFrame to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"csv\").mode('overwrite').save('output')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
